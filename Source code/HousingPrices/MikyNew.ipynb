{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HousingPrices\n",
    "Implement from scratch the ridge regression algorithm for regression with square loss.\n",
    "\n",
    "Apply the algorithm to the prediction of the label medianHouseValue in this dataset.\n",
    "\n",
    "Study the dependence of the cross-validated risk estimate on the parameter alpha of ridge regression.\n",
    "\n",
    "Try using PCA to improve the risk estimate.\n",
    "\n",
    "Optionally, use nested cross-validated risk estimates to remove the need of choosing the parameter.\n",
    "\n",
    "Here is a description of the attributes in the dataset:\n",
    "1. longitude: A measure of how far west a house is; a higher value is farther west\n",
    "2. latitude: A measure of how far north a house is; a higher value is farther north\n",
    "3. housingMedianAge: Median age of a house within a block; a lower number is a newer building\n",
    "4. totalRooms: Total number of rooms within a block\n",
    "5. totalBedrooms: Total number of bedrooms within a block\n",
    "6. population: Total number of people residing within a block\n",
    "7. households: Total number of households, a group of people residing within a home unit, for a block\n",
    "8. medianIncome: Median income for households within a block of houses (measured in tens of thousands of US Dollars)\n",
    "9. medianHouseValue: Median house value for households within a block (measured in US Dollars)\n",
    "10. oceanProximity: Location of the house w.r.t ocean/sea\n",
    "\n",
    "Note: The dataset has an attribute with missing values and an attribute with categorical values. Find a way of handling these anomalies and justify your choice.\n",
    "\n",
    "\n",
    "## License\n",
    "Copyright 2020 (c) Anna Olena Zhab'yak, Michele Maione. All rights reserved.\n",
    "\n",
    "Licensed under the [MIT](LICENSE) License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------------\n",
    "# Ridge regression\n",
    "This is a regression technique wich allows to deal with the multicollinearity of the features and try to reduce the overfitting of the data. \n",
    "minimize two elements, the RSS and the objective function \n",
    "\n",
    "In ridge regression two elements are optimized , the RSS and the objective function\n",
    "\n",
    "$ \\lVert y - Xw \\rVert ^2_2 + \\alpha \\lVert w \\rVert^2_2 $\n",
    "\n",
    "It is a variation of the Linear regression with a tunning parameter alpha to control model complexity.\n",
    "need to do a trade off between variance and bias\n",
    "1. the inhability for the model to capture the true relationship is called bias. How well the model fits our data \n",
    "2. how well model does on a completely new dataset. \n",
    "\n",
    "use the hyper parameter called alpha , to add a bias , prevent the model from real, it will have better accurancy on the testing set, it means it will not overfit in the training set.  \n",
    "\n",
    "## Libraries \n",
    "\n",
    "For this analysis, we will use a few libraries for managing and transform the database as well as to implement the ridge regression. First, let's download the libraries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "No module named numpy",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-760e5d49e1e9>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpandas\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mseaborn\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mImportError\u001b[0m: No module named numpy"
     ]
    }
   ],
   "source": [
    "import numpy\n",
    "import pandas\n",
    "import seaborn\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import LabelEncoder, MinMaxScaler\n",
    "from sklearn.model_selection import learning_curve, validation_curve, cross_val_score, train_test_split, KFold, GridSearchCV\n",
    "\n",
    "from LinearRegression.RidgeRegression.cholesky import Cholesky\n",
    "from LinearRegression.RidgeRegression.svd import SVD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------------\n",
    "## Dataset analysis\n",
    "Open the database, check the structure, show the head, take a look of its structure, the descriptive statistics and manage the columns.\n",
    "\n",
    "Create the constants\n",
    "\n",
    "The target value is the median_house_value which is predicted based on the features of different houses."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mean squared error\n",
    "The mean squared error function computes mean square error, a risk metric corresponding to the expected value of the squared (quadratic) error or loss.\n",
    "\n",
    "### R²\n",
    "The coefficient R² is defined as $ 1 - {u \\over v} $, where u is the residual sum of squares $ \\sum (y - y')^2 $ and v is the total sum of squares $ \\sum (y - \\bar{y})^2 $.\n",
    "\n",
    "The best possible score is 1.0 and it can be negative (because the model can be arbitrarily worse).\n",
    "\n",
    "A constant model that always predicts the expected value of y, disregarding the input features, would get a R² score of 0.0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "RMS = 'neg_mean_squared_error'\n",
    "R2 = 'r2'\n",
    "\n",
    "#scoring = R2\n",
    "#scoring_neg = 1\n",
    "#scoring_label = 'R²'\n",
    "\n",
    "scoring = RMS\n",
    "scoring_neg = -1\n",
    "scoring_label = 'MSE'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "nested_cross_validation_trials = 10\n",
    "\n",
    "fit_intercept = True\n",
    "normalize = True\n",
    "\n",
    "shuffleDataSet = False\n",
    "\n",
    "cpu = -1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Valori di ɑ usati"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "alphas = numpy.linspace(1e-5, 1, 20)\n",
    "alphas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Create the constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "column_to_predict = 'median_house_value'\n",
    "categories_columns = ['ocean_proximity']\n",
    "numerics_columns = [\"longitude\", \"latitude\", \"housing_median_age\", \"total_rooms\", \"total_bedrooms\", \"population\", \"households\", \"median_income\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "data_frame = pandas.read_csv(filepath_or_buffer='cal-housing.csv')\n",
    "\n",
    "# shuffle all DB\n",
    "data_frame = data_frame.sample(frac=1).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "data_frame.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "data_frame.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "data_frame.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is the distribution of the values of the target variable \"median_house_value\".\n",
    "It looks like a normal distribution with a group of outliers on the highest value of the house."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 7))\n",
    "plt.title('Distribuzione dei valori di \"median_house_value\"')\n",
    "\n",
    "seaborn.distplot(data_frame[column_to_predict])\n",
    "\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------------\n",
    "## Data pre-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outliers = data_frame[data_frame[column_to_predict] == 500001].index\n",
    "\n",
    "print(f'There are {len(outliers)} outliers')\n",
    "\n",
    "data_frame.drop(outliers, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 7))\n",
    "plt.title('Distribuzione dei valori di \"median_house_value\"')\n",
    "\n",
    "seaborn.distplot(data_frame[column_to_predict])\n",
    "\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Missing values\n",
    "The dataset contains missing values which are handeled by replacing the value with the mean value of the column. This procedure is necessary to avoid errors in the execution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "for c in data_frame.columns:\n",
    "    if data_frame[c].hasnans:\n",
    "        m = data_frame[c].mean()\n",
    "        data_frame[c].fillna(value=m, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Categorical feature\n",
    "Categorical values cannot be treated as such for the statistical analysis, therefore they must be transormed in numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "labelencoder = LabelEncoder()\n",
    "\n",
    "for c in categories_columns:\n",
    "    data_frame[c + '_cat'] = labelencoder.fit_transform(data_frame[c])\n",
    "    \n",
    "data_frame.drop(columns=categories_columns, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correlation of the dataset\n",
    "Explore the Peasrson's coefficient of correlation and build a simmetric correlation matrix between the features. This procedure is helpeful to eventually reduce the dimension of the dataset. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Equazioni esempio:\n",
    "\n",
    "$$ \\hat{Y} = \\hat{\\beta}_{0} + \\sum \\limits _{j=1} ^{p} X_{j}\\hat{\\beta}_{j}  $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "corr = data_frame.corr()\n",
    "\n",
    "plt.figure(figsize=(15, 15))\n",
    "plt.title('Correlazione tra le caratteristiche')\n",
    "\n",
    "seaborn.heatmap(corr, square=True, annot=True)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pearson's coefficient is determined by :\n",
    "\n",
    "$$ r_{i,j} = \\frac{\\sum_{t=1}^m (x_{i,t}-\\mu_i)(x_{j,t}-\\mu_j)}{\\sqrt{\\sum_{t=1}^m (x_{i,t}-\\mu_i)^2}\\sqrt{\\sum_{t=1}^m (x_{j,t}-\\mu_j)^2}} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The coefficient is in between -1 and +1, when it is close to |1| then there is a correlation (positive or negative) otherwise if the coefficient is close to 0 there isn't any correlation. If some features are linearly correlated they are not useful, because we can explain one feature through the correlated one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "correlated_features = corr[((corr > 0.75) | (corr < -0.75)) & (corr != 1.0)].dropna(axis='index', how='all')\n",
    "correlated_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see the pairs of the most correlated features are \n",
    "(households - total_bedrooms)\n",
    "(population - housegìholds)\n",
    "(total_rooms - total_bedrooms)\n",
    "(total_bedroom - households)\n",
    "\n",
    "Delete the correlated features\n",
    "\n",
    "1. population \n",
    "2. total_rooms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "columns_to_remove = ['latitude', 'population', 'total_rooms', column_to_predict]\n",
    "columns_to_use = list(data_frame.columns)\n",
    "\n",
    "for u in columns_to_remove:\n",
    "    columns_to_use.remove(u)\n",
    "    if numerics_columns.count(u) > 0:\n",
    "        numerics_columns.remove(u)\n",
    "\n",
    "y = data_frame[column_to_predict]\n",
    "X = data_frame.drop(columns=columns_to_remove)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quindi la nostra attuale X è la seguente:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quindi la nostra attuale y è la seguente:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------------\n",
    "## Scaler\n",
    "Transform features by scaling each feature to a given range.\n",
    "\n",
    "This estimator scales and translates each feature individually such that it is in the given range on the training set, e.g. between zero and one.\n",
    "\n",
    "This transformation is often used as an alternative to zero mean, unit variance scaling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_to_predict_idx = data_frame.columns.get_loc(column_to_predict)\n",
    "cols = list(range(0, data_frame.shape[1]))\n",
    "cols.remove(column_to_predict_idx)\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "scaler.fit(data_frame)\n",
    "data_frame = scaler.transform(data_frame)\n",
    "\n",
    "X = data_frame[:, cols]\n",
    "y = data_frame[:, column_to_predict_idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------------\n",
    "## Learning algorithm\n",
    "Before starting to learn the algorithm, the dataset must be splitted in subsets in order to preserve some data for the validation of the method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "cholesky = Cholesky()\n",
    "svd = SVD()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train & Test\n",
    "Train 60%, Test 40%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.4, shuffle=shuffleDataSet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Elaborazione per range di ɑ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "maes_c = []\n",
    "maes_s = []\n",
    "\n",
    "r2s_c = []\n",
    "r2s_s = []\n",
    "\n",
    "for ɑ in alphas:\n",
    "    cholesky.alpha = ɑ\n",
    "    svd.alpha = ɑ\n",
    "\n",
    "    cholesky.fit(x_train, y_train)\n",
    "    svd.fit(x_train, y_train)\n",
    "    \n",
    "    y_predict_c = cholesky.predict(x_test)\n",
    "    y_predict_s = svd.predict(x_test)\n",
    "\n",
    "    # Return the MSE of the prediction.\n",
    "    mae_c = mean_squared_error(y_predict_c, y_test)\n",
    "    mae_s = mean_squared_error(y_predict_s, y_test)\n",
    "    \n",
    "    maes_c.append(mae_c)\n",
    "    maes_s.append(mae_s)\n",
    "    \n",
    "    r2_c = r2_score(y_predict_c, y_test)\n",
    "    r2_s = r2_score(y_predict_s, y_test)\n",
    "    \n",
    "    r2s_c.append(r2_c)\n",
    "    r2s_s.append(r2_s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 7))\n",
    "\n",
    "plt.plot(alphas, maes_c, label='Cholesky')\n",
    "plt.plot(alphas, maes_s, label='SVD')\n",
    "\n",
    "plt.xlabel(\"ɑ\")\n",
    "plt.ylabel(\"MSE\")\n",
    "\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 7))\n",
    "\n",
    "plt.plot(alphas, r2s_c, label='Cholesky')\n",
    "plt.plot(alphas, r2s_s, label='SVD')\n",
    "\n",
    "plt.xlabel(\"ɑ\")\n",
    "plt.ylabel(\"R²\")\n",
    "\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "idx_max_r2_c = numpy.argmax(r2s_c)\n",
    "idx_max_r2_s = numpy.argmax(r2s_s)\n",
    "\n",
    "idx_max_mae_c = numpy.argmax(maes_c)\n",
    "idx_max_mae_s = numpy.argmax(maes_s)\n",
    "\n",
    "best_alpha_c = alphas[idx_max_r2_c]\n",
    "best_alpha_s = alphas[idx_max_r2_s]\n",
    "\n",
    "print(f'Cholesky best ɑ: {best_alpha_c}')\n",
    "print(f'Cholesky best MAE: {maes_c[idx_max_mae_c]}')\n",
    "print(f'Cholesky best R²: {maes_c[idx_max_r2_c]}')\n",
    "\n",
    "print(f'SVD best ɑ: {best_alpha_s}')\n",
    "print(f'SVD best MAE: {r2s_s[idx_max_r2_s]}')\n",
    "print(f'SVD best R²: {r2s_c[idx_max_mae_s]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cholesky.alpha = best_alpha_c\n",
    "svd.alpha = best_alpha_c\n",
    "\n",
    "cholesky.fit(x_train, y_train)\n",
    "svd.fit(x_train, y_train)\n",
    "\n",
    "y_predict_c = cholesky.predict(x_test)\n",
    "y_predict_s = svd.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(2, figsize=(15, 15))\n",
    "\n",
    "ax1.set_title('Real vs Predicted - Cholesky')\n",
    "ax2.set_title('Real vs Predicted - SVD')\n",
    "\n",
    "seaborn.regplot(y_predict_c, y_test, scatter_kws={'alpha':0.5}, ax=ax1)\n",
    "seaborn.regplot(y_predict_s, y_test, scatter_kws={'alpha':0.5}, ax=ax2)\n",
    "\n",
    "ax1.set_xlabel('Predicted')\n",
    "ax1.set_ylabel('Real')\n",
    "ax2.set_xlabel('Predicted')\n",
    "ax2.set_ylabel('Real')\n",
    "\n",
    "ax1.grid()\n",
    "ax2.grid()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------------\n",
    "## Cross-validation\n",
    "\n",
    "### Cross-validated risk estimate\n",
    "Cross Validation is is a method that allows to perform selection and  validation of the learning algorithm. \n",
    "\n",
    "Set aside a portion of the data for the test set and perform multiple interations on the training data, splitted in k-folds and validate the hyperparameter to one fold per time . Then compare to the test data.\n",
    "\n",
    "Method for comparing is the root mean squared error.\n",
    "\n",
    "Normalize to get values normalized\n",
    "\n",
    "Use the 5-fold cross validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### K-Folds cross-validator\n",
    "Provides train/test indices to split data in train/test sets.\n",
    "\n",
    "Split dataset into k consecutive folds (without shuffling by default).\n",
    "\n",
    "Each fold is then used once as a validation while the k - 1 remaining folds form the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_c = []\n",
    "scores_s = []\n",
    "\n",
    "kf = KFold(n_splits=5, shuffle=shuffleDataSet)\n",
    "\n",
    "for ɑ in alphas:\n",
    "    k_scores_c = []\n",
    "    k_scores_s = []\n",
    "    \n",
    "    cholesky.alpha = ɑ\n",
    "    svd.alpha = ɑ\n",
    "    \n",
    "    for train_index, test_index in kf.split(X):\n",
    "        x_train, x_test = X[train_index], X[test_index]\n",
    "        y_train, y_test = y[train_index], y[test_index]\n",
    "                \n",
    "        cholesky.fit(x_train, y_train)\n",
    "        svd.fit(x_train, y_train)\n",
    "\n",
    "        y_pred_c = cholesky.predict(x_test)\n",
    "        y_pred_s = svd.predict(x_test)\n",
    "        \n",
    "        # Return the coefficient of determination R² of the prediction.\n",
    "        score_c = r2_score(y_test, y_pred_s)\n",
    "        score_s = r2_score(y_test, y_pred_c)\n",
    "\n",
    "        k_scores_c.append(score_c)\n",
    "        k_scores_s.append(score_s)\n",
    "        \n",
    "    scores_c.append(numpy.mean(k_scores_c))\n",
    "    scores_s.append(numpy.mean(k_scores_s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx_max_c = numpy.argmax(scores_c)\n",
    "idx_max_s = numpy.argmax(scores_s)\n",
    "\n",
    "best_alpha_c = alphas[idx_max_c]\n",
    "best_alpha_s = alphas[idx_max_s]\n",
    "\n",
    "print(f'Cholesky best ɑ: {best_alpha_c}')\n",
    "print(f'Cholesky best score: {scores_c[idx_max_c]}')\n",
    "\n",
    "print(f'SVD best ɑ: {best_alpha_s}')\n",
    "print(f'SVD best score: {scores_s[idx_max_s]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 7))\n",
    "plt.xlabel('ɑ')\n",
    "plt.ylabel('R²')\n",
    "\n",
    "plt.plot(alphas, scores_c, label='Cholesky')\n",
    "plt.plot(alphas, scores_s, label='SVD')\n",
    "\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "idx_max_c = numpy.argmax(scores_c)\n",
    "idx_max_s = numpy.argmax(scores_s)\n",
    "\n",
    "if best_alpha_c == alphas[idx_max_c]:\n",
    "    print('Il valore di ɑ per Cholesky è invariato!')\n",
    "else:    \n",
    "    best_alpha_c = alphas[idx_max_c]\n",
    "    print(f'Cholesky best ɑ: {best_alpha_c}')\n",
    "\n",
    "if best_alpha_s == alphas[idx_max_s]:\n",
    "    print('Il valore di ɑ per SVD è invariato!')\n",
    "else:    \n",
    "    best_alpha_s = alphas[idx_max_s]\n",
    "    print(f'SVD best ɑ: {best_alpha_s}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "if best_alpha_c != alphas[idx_max_c]:\n",
    "    cholesky.set_params(alpha = best_alpha_c)\n",
    "    cholesky.fit(x_train, y_train, best_alpha_c)\n",
    "    y_predict_c = cholesky.predict(x_test)\n",
    "    score_c = mean_squared_error(y_predict_c, y_test)    \n",
    "    print(f'Cholesky score: {score_c}')\n",
    "\n",
    "if best_alpha_s != alphas[idx_max_s]:\n",
    "    svd.set_params(alpha = best_alpha_s)\n",
    "    svd.fit(x_train, y_train, best_alpha_s)\n",
    "    y_predict_s = svd.predict(x_test)\n",
    "    score_s = mean_squared_error(y_predict_s, y_test)\n",
    "    print(f'SVD score: {score_s}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if best_alpha_c != alphas[idx_max_c] or best_alpha_s != alphas[idx_max_s]:\n",
    "    fig, (ax1, ax2) = plt.subplots(2, figsize=(15, 15))\n",
    "\n",
    "    ax1.set_title('Real vs Predicted - Cholesky')\n",
    "    ax2.set_title('Real vs Predicted - SVD')\n",
    "\n",
    "    seaborn.regplot(y_predict_c, y_test, scatter_kws={'alpha':0.5}, ax=ax1)\n",
    "    seaborn.regplot(y_predict_s, y_test, scatter_kws={'alpha':0.5}, ax=ax2)\n",
    "\n",
    "    ax1.set_xlabel('Predicted')\n",
    "    ax1.set_ylabel('Real')\n",
    "    ax2.set_xlabel('Predicted')\n",
    "    ax2.set_ylabel('Real')\n",
    "\n",
    "    ax1.grid()\n",
    "    ax2.grid()\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------------\n",
    "## Principal component analysis (PCA).\n",
    "\n",
    "Linear dimensionality reduction using Singular Value Decomposition of the data to project it to a lower dimensional space. The input data is centered but not scaled for each feature before applying the SVD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_ts = int(X.shape[0] * 0.1)\n",
    "max_ts = int(X.shape[0] * 0.8)\n",
    "step_ts = int(X.shape[0] * 0.05)\n",
    "sizes = range(min_ts, max_ts, step_ts)\n",
    "\n",
    "print(sizes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coef_list = []\n",
    "\n",
    "cholesky.alpha = best_alpha_c\n",
    "\n",
    "for s in sizes:\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4, shuffle=shuffleDataSet)\n",
    "    cholesky.fit(X_train, y_train)    \n",
    "    coef_list.append(cholesky.coef_)\n",
    "\n",
    "coef_matrix = numpy.array(coef_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=2)\n",
    "pca.fit(coef_matrix)\n",
    "coef_pca = pca.transform(coef_matrix)\n",
    "\n",
    "plt.figure(figsize=(15, 7))\n",
    "plt.grid()\n",
    "plt.scatter(coef_pca[:,0], coef_pca[:,1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=X.shape[1])\n",
    "pca.fit(X)\n",
    "\n",
    "X_pca = pca.transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 7))\n",
    "plt.title('PCA')\n",
    "\n",
    "plt.plot(pca.singular_values_, label='Singular values')\n",
    "\n",
    "plt.grid()\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We project all the datapoints on the five principal components. Then we check the learning curve again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=5)\n",
    "pca.fit(X)\n",
    "\n",
    "X_pca = pca.transform(X)\n",
    "\n",
    "# Returns:\n",
    "# -Numbers of training examples that has been used to generate the learning curve. Note that the number of ticks might be less than n_ticks because duplicate entries will be removed;\n",
    "# -Scores on training sets;\n",
    "# -Scores on test set.\n",
    "train_size, train_score, val_score = learning_curve(cholesky, X_pca, y, train_sizes=sizes, cv=5, scoring=scoring, n_jobs=cpu)\n",
    "\n",
    "train_score_mean = scoring_neg * numpy.mean(train_score, axis=1)\n",
    "train_score_std = numpy.std(train_score, axis=1)\n",
    "val_score_mean = scoring_neg * numpy.mean(val_score, axis=1)\n",
    "val_score_std = numpy.std(val_score, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 15))\n",
    "plt.title('PCA Linear Regression')\n",
    "\n",
    "plt.fill_between(sizes, train_score_mean - train_score_std, train_score_mean + train_score_std, alpha=0.1, color=\"r\")\n",
    "plt.fill_between(sizes, val_score_mean - val_score_std, val_score_mean + val_score_std, alpha=0.1, color=\"g\")\n",
    "\n",
    "plt.plot(sizes, train_score_mean, 'o-', color=\"r\", label=\"Training error\")\n",
    "plt.plot(sizes, val_score_mean, 'o-', color=\"g\", label=\"CV risk estimate\")\n",
    "\n",
    "plt.xlabel('Training size')\n",
    "plt.ylabel(scoring_label)\n",
    "\n",
    "plt.grid()\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coef_list = []\n",
    "\n",
    "for s in sizes:\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_pca, y, train_size=s, shuffle=shuffleDataSet)\n",
    "    cholesky.fit(X_train, y_train)\n",
    "    coef_list.append(cholesky.coef_)\n",
    "    \n",
    "coef_matrix = numpy.array(coef_list)\n",
    "\n",
    "pca = PCA(n_components=2)\n",
    "pca.fit(coef_matrix)\n",
    "coef_pca = pca.transform(coef_matrix)\n",
    "\n",
    "plt.figure(figsize=(15, 7))\n",
    "plt.grid()\n",
    "plt.scatter(coef_pca[:,0], coef_pca[:,1])\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
